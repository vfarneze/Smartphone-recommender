{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping https://www.maiscelular.com.br/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main search: https://www.maiscelular.com.br/fichas-tecnicas/?aparelho=1&z=1\n",
    "\n",
    "This website shows tecnical information about many cellphones. Its search page shows only 20 devices and (as of 2020-05-17) had\n",
    "about 4923 devices in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best aproach until now was:\n",
    "* 1 - access the main link and get number of pages\n",
    "* 2 - for each page, get all possible infos from cellphones and hiperlinks\n",
    "* 3 - access each of the 4 thousand, access links and get informations!\n",
    "* 4 - store in a dataframe\n",
    "* 5 - Finally, for each link obtained, extract all information from smartphone/cellphone using pd.read_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions and main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction 1: which smartphones are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_url(page, results_per_page=1000):\n",
    "    \"\"\"This function receives a number (page) and returns an url for the website\"\"\"\n",
    "    url1 = 'https://www.maiscelular.com.br/fichas-tecnicas/?aparelho=1'\n",
    "    url2 = f'&z={page}'\n",
    "\n",
    "    url = url1 + url2\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_soup_from_url(url):\n",
    "    \"\"\"This function gets an url and returns a bs4 soup\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        }\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_array_of_pages(soup):\n",
    "    \"\"\"This function receies a bs4 element (soup) from maiscelular.com.br website and returns an array with of pages\n",
    "    to iterate in the future.\"\"\"\n",
    "    number_of_smartphones = soup.find_all('span', attrs={'class':\"d-none d-sm-block\"})[0].text\n",
    "    number_of_smartphones = int(re.findall('\\d+',number_of_smartphones)[0])\n",
    "    pages = np.arange(1,math.floor(number_of_smartphones/20 + 1)+1)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_smartphone_info_from_page(soup):\n",
    "    \"\"\"This function receies a bs4 element (soup) from maiscelular.com.br website and returns\n",
    "    a dataframe with useful information (mainly link to techical characteristics) about all smartphones\n",
    "    from a page.\"\"\"\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    img_name_infos = soup.find_all('div', attrs={'class':\"bl1\"})\n",
    "    mini_fichas_tecnicas = soup.find_all('div', attrs={'class':\"col-md-6 col-lg-6 resumo\"})\n",
    "\n",
    "    for info, ficha in zip(img_name_infos, mini_fichas_tecnicas):\n",
    "        link = 'https://www.maiscelular.com.br/' + info.find_all('a')[0]['href']\n",
    "        image_link = 'https:' + info.find_all('img')[0]['src']\n",
    "        name = info.find_all('strong')[0].text\n",
    "        screen = ficha.find_all('div')[0].text\n",
    "        camera = ficha.find_all('div')[1].text\n",
    "        mram_pross = ficha.find_all('div')[2].text\n",
    "        batery = ficha.find_all('div')[3].text\n",
    "\n",
    "        minidf = pd.DataFrame({'name':name,\n",
    "                               'image_link':image_link,\n",
    "                               'link':link,\n",
    "                               'screen':screen,\n",
    "                               'camera':camera,\n",
    "                               'processor_RAM':mram_pross,\n",
    "                               'batery':batery},index=[0])\n",
    "        results = pd.concat([results, minidf])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_smartphone_info_from_all_pages(pages):\n",
    "    \"\"\"This function is the main function for getting all products information from maiscelular.com.\n",
    "    It receives an array of pages, iterates over it and gets all bs4 elements (soup) and extracts information,\n",
    "    returning a dataframe\"\"\"\n",
    "\n",
    "#     results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for page in tqdm(pages):\n",
    "        url = get_url(page)\n",
    "        soup = get_soup_from_url(url)\n",
    "        results = pd.concat([results, get_smartphone_info_from_page(soup)])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mainfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exctract_from_maiscelular():\n",
    "    \"\"\"This function executes all functions and data from the search pages. Each product is a smartphone, and\n",
    "    the link to it will give more detailed information. This function returns a dataframe.\"\"\"\n",
    "    url = get_url(1)\n",
    "    soup = get_soup_from_url(url)\n",
    "    pages = get_array_of_pages(soup)\n",
    "    print(f'number of pages: {pages[-1]}')\n",
    "    time.sleep(1)\n",
    "    results = get_smartphone_info_from_all_pages(pages).reset_index(drop=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = exctract_from_maiscelular()\n",
    "results = pd.read_csv('storage/'+'maiscelular_backup_main_page2020y-5m-16d-22h.csv', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['price_link'] = results.loc[:,'link'].apply(lambda x: 'https://www.maiscelular.com.br//fichas-tecnicas' + '/'.join(x.split('fichas-tecnicas')[1].split('/')[0:3]) + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction 2: tecnical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_accentuation_from_cols(dataframe):\n",
    "    \"\"\"This function removes accentuation from columns names. Receives a dataframe and returns the treated DataFrame\"\"\"\n",
    "\n",
    "    \n",
    "    from unidecode import unidecode\n",
    "    \n",
    "    unidecode_dict = {}\n",
    "    for col in dataframe.columns.tolist():\n",
    "        unidecode_dict[col] = unidecode(col)\n",
    "    dataframe.rename(columns=unidecode_dict, inplace=True)        \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_missing_columns(dataframe1, dataframe2):\n",
    "    \"\"\"THis function receives a dataframe1 and a dataframe2, and checks if there is a column in dataframe1\n",
    "    that is not in datafram2. If so, it creates the missing column in dataframe2 with 'sem registro' as value\n",
    "    and reorders the dataframe1 to have the same columns order.\"\"\"\n",
    "    \n",
    "    #verify if all columns match. If not, add the column...\n",
    "    cols_not_in_search = []\n",
    "    for col in dataframe1.columns:\n",
    "        if col not in dataframe2.columns:\n",
    "#             print('Propriedade n√£o encontrada: ', col)\n",
    "            dataframe2[col] = 'sem registro'\n",
    "\n",
    "    return dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reorder_columns(dataframe1, dataframe2):\n",
    "    \"\"\"This function receives two dataframes, compare their columns order, and return the second\n",
    "    adjusted with the same order of the first.\"\"\"\n",
    "    \n",
    "    while dataframe1.columns.tolist() != dataframe2.columns.tolist():\n",
    "        print('looks like dataframe1 and dataframe2 have columns ordered in a different way...')\n",
    "        for col_df1, col_df2 in zip(dataframe1.columns, dataframe2.columns):\n",
    "            if col_df1 != col_df2:\n",
    "                print('reorganizing...')\n",
    "                column_to_be_put_in_the_end = dataframe2.loc[:,col_df2]\n",
    "                dataframe2.drop(columns=col_df2, inplace=True)\n",
    "                dataframe2[col_df2] = column_to_be_put_in_the_end\n",
    "                break\n",
    "        print('checking again...')\n",
    "    return dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rename_duplicate_columns(dataframe):\n",
    "\n",
    "    lista = dataframe.columns.tolist()\n",
    "    checklist = []\n",
    "\n",
    "    for element in lista:\n",
    "\n",
    "        if element not in checklist:\n",
    "            checklist.append(element)\n",
    "        else:\n",
    "            checklist.append(element + '.1')\n",
    "\n",
    "    dataframe.columns = checklist\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### mainfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_data_from_each_smartphone_link(first_results, backup_path='smartphones_backup.csv'):\n",
    "    \"\"\"This function receives a dataframe result from the extractions of the main pages of smartphones from\n",
    "    maiscelular.com and extract information from each smartphone and returns it as a dataframe.\n",
    "    It also saves a dataframe in a backup. It there is no backup, it starts from the beginning,\n",
    "    however, in the contrary case, it loads the backup and restarts from where it stopped.\"\"\"\n",
    "    \n",
    "    # load backup\n",
    "    backup = pd.read_csv(backup_path, encoding='utf-8')\n",
    "    backup = remove_accentuation_from_cols(backup)\n",
    "    \n",
    "    new_results = backup.copy()\n",
    "    \n",
    "    # check which links will be searched\n",
    "    links_already_searched = backup.link.unique()\n",
    "    mask = first_results.link.apply(lambda x: x not in links_already_searched)\n",
    "    links_not_searched = first_results.loc[mask].link\n",
    "    \n",
    "    if len(links_not_searched) == 0:\n",
    "        print('There are no new smartphones to extract!')\n",
    "        return backup \n",
    "    \n",
    "    print(f'Extracting information from {len(links_not_searched)} new smartphones...')\n",
    "    time.sleep(1)    \n",
    "    for link in tqdm(links_not_searched):\n",
    "        df_html = pd.read_html(link,encoding='utf-8')[0].drop(columns=[2,3,4])\n",
    "\n",
    "        #removing rows that are the divisros of the extracted table.\n",
    "        smartphone_name = df_html.loc[0][1]\n",
    "        indexes_to_remove = df_html.loc[df_html[1] == smartphone_name].index.drop(0)\n",
    "        df_html.drop(indexes_to_remove,inplace=True)\n",
    "\n",
    "        #now we transpose the dataframe and rename the first column which is the smartphone name\n",
    "        df_html = df_html.set_index(0).T.rename(columns={'Caracter√≠sticas':'Smartphone'})\n",
    "\n",
    "        #adding link\n",
    "        df_html['link'] = link\n",
    "        \n",
    "        #treat new result, remove accentuation and reset_index\n",
    "        df_html = remove_accentuation_from_cols(df_html)\n",
    "        df_html = rename_duplicate_columns(df_html)\n",
    "        df_html = add_missing_columns(new_results, df_html)\n",
    "        \n",
    "        # add to the final result and backup\n",
    "        new_results = pd.concat([new_results, df_html], ignore_index=True).reset_index(drop=True)\n",
    "        new_results.to_csv(backup_path, encoding='utf-8', index=False)\n",
    "    \n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Extract results and prices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no new smartphones to extract!\n"
     ]
    }
   ],
   "source": [
    "new_results= extract_data_from_each_smartphone_link(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction 3: prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, its better to separate the extractions (tecnical data and price) , because one uses pd.read_html() and the other will use BeautifulSoup() (respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1441 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441  links left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1441/1441 [36:25<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('backupprices.csv', encoding='utf-8')\n",
    "tosearch = df.loc[df.name.apply(lambda x: type(x) == float)]\n",
    "indexes = tosearch.index\n",
    "print(len(indexes), ' links left')\n",
    "\n",
    "for index in tqdm(indexes):\n",
    "    link = df.loc[index,'price_link']\n",
    "    soup = get_soup_from_url(link)\n",
    "    df.loc[index,'name'] = soup.find_all('h1', attrs={'itemprop':\"headline\"})[0].text\n",
    "    try:\n",
    "        df.loc[index,'best_price'] = soup.find_all('a', attrs={'class':\"best\"})[0].text\n",
    "    except:\n",
    "        df.loc[index,'best_price'] = 'sem_preco'\n",
    "    df.to_csv('backupprices.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: Merge everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv('backupprices.csv', encoding='utf-8')\n",
    "prices.loc[:,'name'] = prices.loc[:,'name'].apply(lambda x: x.strip(' Ficha T√©cnica'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartphones = pd.read_csv('smartphones_backup.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresults = pd.concat([smartphones, prices.drop(columns='name')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format best_price column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'R$'\n",
    "finalresults.loc[:,'best_price'] = finalresults.loc[:,'best_price'].apply(lambda x: x.split('R$')[1] if len(x.split('R$')) > 1 else x)\n",
    "\n",
    "# remove dots\n",
    "finalresults.loc[:,'best_price'] = finalresults.loc[:,'best_price'].apply(lambda x: x.replace('.','') if '.' in x else x)\n",
    "finalresults.loc[:,'best_price'] = finalresults.loc[:,'best_price'].apply(lambda x: np.nan if x == 'sem_preco' else x)\n",
    "\n",
    "#turn values from string to floats\n",
    "finalresults.loc[:,'best_price'] = finalresults.loc[:,'best_price'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresults.to_csv('smartphones.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
